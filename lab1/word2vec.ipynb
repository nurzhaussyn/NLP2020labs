{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word2vec.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Kx5X-m5AK2c9","colab_type":"code","colab":{}},"source":["import torch\n","from torch.autograd import Variable\n","import numpy as np\n","import torch.functional as F\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-DKzGdnoIsAV","colab_type":"code","colab":{}},"source":["\n","corpus = [\n","    'he is a king',\n","    'she is a queen',\n","    'he is a man',\n","    'she is a woman',\n","    'warsaw is poland capital',\n","    'berlin is germany capital',\n","    'paris is france capital',\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fn2cMtniJigl","colab_type":"code","outputId":"22b62813-9383-416d-90c8-4d58bd919bb5","executionInfo":{"status":"ok","timestamp":1568891817546,"user_tz":-360,"elapsed":685,"user":{"displayName":"Александр Пак","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoApcWJgCi9HMJv_dkQDj6kUy23W-cM3CGm_lX=s64","userId":"15550640353612535544"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["def tokenize_corpus(corpus):\n","    tokens = [x.split() for x in corpus]\n","    return tokens\n","\n","tokenized_corpus = tokenize_corpus(corpus)\n","print(tokenized_corpus)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'man'], ['she', 'is', 'a', 'woman'], ['warsaw', 'is', 'poland', 'capital'], ['berlin', 'is', 'germany', 'capital'], ['paris', 'is', 'france', 'capital']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S7GDVdmtKGG7","colab_type":"code","colab":{}},"source":["vocabulary = []\n","for sentence in tokenized_corpus:\n","    for token in sentence:\n","        if token not in vocabulary:\n","            vocabulary.append(token)\n","\n","word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n","idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n","\n","vocabulary_size = len(vocabulary)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q78TRvzUKQRd","colab_type":"code","outputId":"6433da10-9703-4dca-9bba-e6c22d644687","executionInfo":{"status":"ok","timestamp":1568891818855,"user_tz":-360,"elapsed":576,"user":{"displayName":"Александр Пак","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoApcWJgCi9HMJv_dkQDj6kUy23W-cM3CGm_lX=s64","userId":"15550640353612535544"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["word2idx"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'a': 2,\n"," 'berlin': 11,\n"," 'capital': 10,\n"," 'france': 14,\n"," 'germany': 12,\n"," 'he': 0,\n"," 'is': 1,\n"," 'king': 3,\n"," 'man': 6,\n"," 'paris': 13,\n"," 'poland': 9,\n"," 'queen': 5,\n"," 'she': 4,\n"," 'warsaw': 8,\n"," 'woman': 7}"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"m_FnNckvKUXI","colab_type":"code","colab":{}},"source":["window_size = 2\n","idx_pairs = []\n","# for each sentence\n","for sentence in tokenized_corpus:\n","    indices = [word2idx[word] for word in sentence]\n","    # for each word, threated as center word\n","    for center_word_pos in range(len(indices)):\n","        # for each window position\n","        for w in range(-window_size, window_size + 1):\n","            context_word_pos = center_word_pos + w\n","            # make soure not jump out sentence\n","            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n","                continue\n","            context_word_idx = indices[context_word_pos]\n","            idx_pairs.append((indices[center_word_pos], context_word_idx))\n","\n","idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pRBS-TAKvEA","colab_type":"code","outputId":"e93a34ce-834f-4e9b-919d-ed0f64f5b5ee","executionInfo":{"status":"ok","timestamp":1568891821660,"user_tz":-360,"elapsed":466,"user":{"displayName":"Александр Пак","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoApcWJgCi9HMJv_dkQDj6kUy23W-cM3CGm_lX=s64","userId":"15550640353612535544"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["idx_pairs[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1],\n","       [0, 2],\n","       [1, 0],\n","       [1, 2],\n","       [1, 3],\n","       [2, 0],\n","       [2, 1],\n","       [2, 3],\n","       [3, 1],\n","       [3, 2]])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"R76JAnrXLK8s","colab_type":"text"},"source":["![alt text](https://miro.medium.com/max/377/1*uYiqfNrUIzkdMrmkBWGMPw.png)"]},{"cell_type":"code","metadata":{"id":"5Xsy5_fAK-qK","colab_type":"code","colab":{}},"source":["def get_input_layer(word_idx):\n","    x = torch.zeros(vocabulary_size).float()\n","    x[word_idx] = 1.0\n","    return x\n","  \n","  #Input layer is just the center word encoded in one-hot manner. It dimensions are [1, vocabulary_size]\n","  \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLU--gGwLNLX","colab_type":"code","outputId":"cd2921e0-6c5e-4d88-abfd-ce6a3ae3777c","executionInfo":{"status":"ok","timestamp":1568892069260,"user_tz":-360,"elapsed":22868,"user":{"displayName":"Александр Пак","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoApcWJgCi9HMJv_dkQDj6kUy23W-cM3CGm_lX=s64","userId":"15550640353612535544"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["embedding_dims = 5\n","W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n","W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n","num_epochs = 1010\n","learning_rate = 0.001\n","\n","for epo in range(num_epochs):\n","    loss_val = 0\n","    for data, target in idx_pairs:\n","        x = Variable(get_input_layer(data)).float()\n","        y_true = Variable(torch.from_numpy(np.array([target])).long())\n","\n","        z1 = torch.matmul(W1, x)\n","        z2 = torch.matmul(W2, z1)\n","    \n","        log_softmax = F.log_softmax(z2, dim=0)\n","\n","        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n","        loss_val += loss.data.item()\n","        loss.backward()\n","        W1.data -= learning_rate * W1.grad.data\n","        W2.data -= learning_rate * W2.grad.data\n","\n","        W1.grad.data.zero_()\n","        W2.grad.data.zero_()\n","    if epo % 10 == 0:    \n","        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loss at epo 0: 4.538836489404951\n","Loss at epo 10: 4.053764954635075\n","Loss at epo 20: 3.6870625819478717\n","Loss at epo 30: 3.3981100388935634\n","Loss at epo 40: 3.164670308998653\n","Loss at epo 50: 2.9728501898901802\n","Loss at epo 60: 2.8136727179799763\n","Loss at epo 70: 2.681133336680276\n","Loss at epo 80: 2.5708919491086686\n","Loss at epo 90: 2.4794341121401104\n","Loss at epo 100: 2.4036593164716447\n","Loss at epo 110: 2.3407671809196473\n","Loss at epo 120: 2.2882858651024955\n","Loss at epo 130: 2.2441212603024074\n","Loss at epo 140: 2.206563547679356\n","Loss at epo 150: 2.174257889815739\n","Loss at epo 160: 2.1461487650871276\n","Loss at epo 170: 2.1214211889675685\n","Loss at epo 180: 2.099448001384735\n","Loss at epo 190: 2.079744052886963\n","Loss at epo 200: 2.0619311877659388\n","Loss at epo 210: 2.04571270431791\n","Loss at epo 220: 2.030852677140917\n","Loss at epo 230: 2.0171612892832074\n","Loss at epo 240: 2.0044845581054687\n","Loss at epo 250: 1.9926958067076548\n","Loss at epo 260: 1.9816904408591134\n","Loss at epo 270: 1.9713806578091213\n","Loss at epo 280: 1.9616924626486643\n","Loss at epo 290: 1.952563546385084\n","Loss at epo 300: 1.9439408149038042\n","Loss at epo 310: 1.9357779707227434\n","Loss at epo 320: 1.9280357837677002\n","Loss at epo 330: 1.9206792541912623\n","Loss at epo 340: 1.9136783071926662\n","Loss at epo 350: 1.9070060236113413\n","Loss at epo 360: 1.9006385343415397\n","Loss at epo 370: 1.894554272719792\n","Loss at epo 380: 1.8887336952345712\n","Loss at epo 390: 1.8831586922918047\n","Loss at epo 400: 1.8778121829032899\n","Loss at epo 410: 1.8726793033736093\n","Loss at epo 420: 1.8677451133728027\n","Loss at epo 430: 1.8629963517189025\n","Loss at epo 440: 1.8584204026630946\n","Loss at epo 450: 1.8540050762040274\n","Loss at epo 460: 1.8497393608093262\n","Loss at epo 470: 1.8456128852707998\n","Loss at epo 480: 1.8416158369609288\n","Loss at epo 490: 1.837739406313215\n","Loss at epo 500: 1.8339748978614807\n","Loss at epo 510: 1.8303151641573225\n","Loss at epo 520: 1.8267529232161386\n","Loss at epo 530: 1.8232817445482528\n","Loss at epo 540: 1.8198958907808576\n","Loss at epo 550: 1.8165895462036132\n","Loss at epo 560: 1.813357971395765\n","Loss at epo 570: 1.81019686290196\n","Loss at epo 580: 1.8071019836834499\n","Loss at epo 590: 1.80406961951937\n","Loss at epo 600: 1.8010966965130397\n","Loss at epo 610: 1.7981803962162564\n","Loss at epo 620: 1.7953174846512931\n","Loss at epo 630: 1.7925058875765119\n","Loss at epo 640: 1.7897434268678938\n","Loss at epo 650: 1.7870283552578516\n","Loss at epo 660: 1.784358423096793\n","Loss at epo 670: 1.7817321590014867\n","Loss at epo 680: 1.7791483640670775\n","Loss at epo 690: 1.7766056980405536\n","Loss at epo 700: 1.7741028325898307\n","Loss at epo 710: 1.7716387782778058\n","Loss at epo 720: 1.7692125371524265\n","Loss at epo 730: 1.7668230874197823\n","Loss at epo 740: 1.7644697444779531\n","Loss at epo 750: 1.7621517539024354\n","Loss at epo 760: 1.759868129662105\n","Loss at epo 770: 1.7576186622892107\n","Loss at epo 780: 1.7554024696350097\n","Loss at epo 790: 1.7532191259520395\n","Loss at epo 800: 1.7510681561061314\n","Loss at epo 810: 1.7489487358501978\n","Loss at epo 820: 1.7468603134155274\n","Loss at epo 830: 1.7448029552187239\n","Loss at epo 840: 1.7427758540425982\n","Loss at epo 850: 1.7407786573682513\n","Loss at epo 860: 1.7388109956468856\n","Loss at epo 870: 1.7368725453104292\n","Loss at epo 880: 1.7349626864705767\n","Loss at epo 890: 1.7330809814589365\n","Loss at epo 900: 1.7312270709446498\n","Loss at epo 910: 1.729400599002838\n","Loss at epo 920: 1.7276011977876935\n","Loss at epo 930: 1.7258284756115505\n","Loss at epo 940: 1.7240819914000376\n","Loss at epo 950: 1.7223613364355905\n","Loss at epo 960: 1.7206661581993103\n","Loss at epo 970: 1.7189961774008615\n","Loss at epo 980: 1.717350605555943\n","Loss at epo 990: 1.7157293677330017\n","Loss at epo 1000: 1.7141318900244578\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JVX9o_vwPBby","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}